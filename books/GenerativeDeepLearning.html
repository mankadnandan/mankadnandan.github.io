<script src="https://rawcdn.githack.com/oscarmorrison/md-page/232e97938de9f4d79f4110f6cfd637e186b63317/md-page.js"></script>

<noscript>

<a href="./books.html" style="text-decoration:none;">&#x1F519;</a>&nbsp; &nbsp;</br></br>

Assimilation of Generative Deep Learning
==============
Teaching Machines to Paint, Write, Compose and Play.

# Foreword
- Foreword is written by Karl Friston - Professor of Neuroscience, University College London.
- Interestingly he is a neuroscience professor and still reading such interesting books which are very much outside his original subject of study.

# Roadmap
- Book is divided into 3 parts:

    - Part I is the general introduction to generative modelling and deep learning.
        - Chap 1: Generative Modelling
        - Chap 2: Deep Learning
            - Building first example of a multilayer perceptron (MLP) using Keras.
                [*`NM - A Multilayer Perceptron (MLP) is a type of feedforward neural network consisting of fully connected neurons with a nonlinear activation function.`*]
                [*`NM - A Feedforward network is a network where the data flows only in one direction, and it doesn't have a loop (like recurrent connections or convolution layers).`*]
                [*`NM - An activation function is a mathematical transformation used between layers in a neural network to scale the output before passing it on to the next layer.`*]
                [*`NM - A linear activation function transforms the input linearly. Since these functions are linear they impose limitation on the network's ability to learn complex, non-linear relationships.`*]
                [*`NM - Non-linear activation functions are more complex and introduce non-linearity into the network. They are commonly used in the hidden layers of a neural network to enable the network to learn complex, non-linear relationships between inputs and outputs.`*]
            - Also adapt this to include convolutional layers to observe performance difference.
                [*`NM - The term "convolution" in convolutional neural networks (CNNs) relates to its English meaning of folding or blending complex structures together.`*]
                [*`NM - The concept of convolution in CNNs reflects the idea of combining and folding together local pieces of information to build up a complex, detailed understanding of the input data. This layered, hierarchical process is essential for tasks like image recognition, where understanding the whole picture requires blending and analyzing many small parts.`*]

    - Part II - Six key techniques.
        - Chap 3: Variational Auto-encoders (VAE). How VAE's are used to generate images of faces and morph between faces.
        - Chap 4:


<a href="./books.html" style="text-decoration:none;">&#x1F519;</a>&nbsp; &nbsp;</br></br>